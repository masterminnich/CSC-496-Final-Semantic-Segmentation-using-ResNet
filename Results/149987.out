Sender: LSF System <lsfadmin@cudanode>
Subject: Job 149987: <#!/bin/bash;#BSUB -q gpu;#BSUB -o %J.out;#BSUB -e %J.err;#BSUB -R "rusage[ngpus_shared=2]"; cd /home/jm2511a/YOLO; /app/python3/bin/python3 run.py> in cluster <zorro_cluster> Done

Job <#!/bin/bash;#BSUB -q gpu;#BSUB -o %J.out;#BSUB -e %J.err;#BSUB -R "rusage[ngpus_shared=2]"; cd /home/jm2511a/YOLO; /app/python3/bin/python3 run.py> was submitted from host <loginnode> by user <jm2511a> in cluster <zorro_cluster>.
Job was executed on host(s) <cudanode>, in queue <gpu>, as user <jm2511a> in cluster <zorro_cluster>.
</home/jm2511a> was used as the home directory.
</home/jm2511a/YOLO> was used as the working directory.
Started at Results reported on 
Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -q gpu
#BSUB -o %J.out
#BSUB -e %J.err
#BSUB -R "rusage[ngpus_shared=2]"

cd /home/jm2511a/YOLO

/app/python3/bin/python3 run.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   16269.99 sec.
    Max Memory :                                 2268 MB
    Average Memory :                             2261.75 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                130
    Run time :                                   11253 sec.
    Turnaround time :                            11252 sec.

The output (if any) follows:

x_train shape: (50000, 32, 32, 3)
50000 train samples
10000 test samples
y_train shape: (50000, 1)
Learning rate:  0.001
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               
                                                                 batch_normalization_4[0][0]      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 16, 16, 32)   4640        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 16, 16, 32)   128         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 16, 16, 32)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 16, 16, 32)   9248        activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 16, 16, 32)   544         activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 16, 16, 32)   0           conv2d_9[0][0]                   
                                                                 batch_normalization_8[0][0]      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 16, 16, 32)   0           add_3[0][0]                      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 16, 16, 32)   9248        activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 16, 16, 32)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  
__________________________________________________________________________________________________
add_4 (Add)                     (None, 16, 16, 32)   0           activation_8[0][0]               
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 32)   0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 32)   0           activation_10[0][0]              
                                                                 batch_normalization_12[0][0]     
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 8, 8, 64)     18496       activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 8, 8, 64)     0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 8, 8, 64)     36928       activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 8, 8, 64)     2112        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  
__________________________________________________________________________________________________
add_6 (Add)                     (None, 8, 8, 64)     0           conv2d_16[0][0]                  
                                                                 batch_normalization_14[0][0]     
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 8, 8, 64)     0           add_6[0][0]                      
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 8, 8, 64)     0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  
__________________________________________________________________________________________________
add_7 (Add)                     (None, 8, 8, 64)     0           activation_14[0][0]              
                                                                 batch_normalization_16[0][0]     
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 8, 8, 64)     0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  
__________________________________________________________________________________________________
add_8 (Add)                     (None, 8, 8, 64)     0           activation_16[0][0]              
                                                                 batch_normalization_18[0][0]     
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_18[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          
__________________________________________________________________________________________________
dense (Dense)                   (None, 10)           650         flatten[0][0]                    
==================================================================================================
Total params: 274,442
Trainable params: 273,066
Non-trainable params: 1,376
__________________________________________________________________________________________________
Using real-time data augmentation.
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  0.0001
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-05
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  1e-06
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
Learning rate:  5e-07
   32/10000 [..............................] - ETA: 2s - loss: 0.1830 - acc: 0.9688  192/10000 [..............................] - ETA: 3s - loss: 0.3924 - acc: 0.9271  384/10000 [>.............................] - ETA: 2s - loss: 0.4039 - acc: 0.9245  576/10000 [>.............................] - ETA: 2s - loss: 0.3944 - acc: 0.9306  768/10000 [=>............................] - ETA: 2s - loss: 0.3986 - acc: 0.9284  960/10000 [=>............................] - ETA: 2s - loss: 0.4337 - acc: 0.9187 1152/10000 [==>...........................] - ETA: 2s - loss: 0.4229 - acc: 0.9210 1344/10000 [===>..........................] - ETA: 2s - loss: 0.4220 - acc: 0.9219 1536/10000 [===>..........................] - ETA: 2s - loss: 0.4040 - acc: 0.9251 1728/10000 [====>.........................] - ETA: 2s - loss: 0.3999 - acc: 0.9265 1920/10000 [====>.........................] - ETA: 2s - loss: 0.4087 - acc: 0.9245 2112/10000 [=====>........................] - ETA: 2s - loss: 0.4177 - acc: 0.9223 2304/10000 [=====>........................] - ETA: 2s - loss: 0.4311 - acc: 0.9193 2496/10000 [======>.......................] - ETA: 2s - loss: 0.4394 - acc: 0.9183 2688/10000 [=======>......................] - ETA: 2s - loss: 0.4558 - acc: 0.9167 2880/10000 [=======>......................] - ETA: 1s - loss: 0.4479 - acc: 0.9181 3072/10000 [========>.....................] - ETA: 1s - loss: 0.4439 - acc: 0.9193 3264/10000 [========>.....................] - ETA: 1s - loss: 0.4438 - acc: 0.9185 3456/10000 [=========>....................] - ETA: 1s - loss: 0.4432 - acc: 0.9175 3680/10000 [==========>...................] - ETA: 1s - loss: 0.4478 - acc: 0.9155 3872/10000 [==========>...................] - ETA: 1s - loss: 0.4480 - acc: 0.9150 4064/10000 [===========>..................] - ETA: 1s - loss: 0.4449 - acc: 0.9161 4288/10000 [===========>..................] - ETA: 1s - loss: 0.4467 - acc: 0.9151 4480/10000 [============>.................] - ETA: 1s - loss: 0.4462 - acc: 0.9156 4704/10000 [=============>................] - ETA: 1s - loss: 0.4476 - acc: 0.9152 4896/10000 [=============>................] - ETA: 1s - loss: 0.4504 - acc: 0.9146 5088/10000 [==============>...............] - ETA: 1s - loss: 0.4532 - acc: 0.9141 5280/10000 [==============>...............] - ETA: 1s - loss: 0.4553 - acc: 0.9133 5472/10000 [===============>..............] - ETA: 1s - loss: 0.4525 - acc: 0.9137 5664/10000 [===============>..............] - ETA: 1s - loss: 0.4572 - acc: 0.9138 5888/10000 [================>.............] - ETA: 1s - loss: 0.4610 - acc: 0.9136 6112/10000 [=================>............] - ETA: 1s - loss: 0.4606 - acc: 0.9136 6304/10000 [=================>............] - ETA: 0s - loss: 0.4604 - acc: 0.9139 6496/10000 [==================>...........] - ETA: 0s - loss: 0.4560 - acc: 0.9144 6720/10000 [===================>..........] - ETA: 0s - loss: 0.4507 - acc: 0.9158 6912/10000 [===================>..........] - ETA: 0s - loss: 0.4495 - acc: 0.9158 7136/10000 [====================>.........] - ETA: 0s - loss: 0.4475 - acc: 0.9159 7328/10000 [====================>.........] - ETA: 0s - loss: 0.4443 - acc: 0.9168 7552/10000 [=====================>........] - ETA: 0s - loss: 0.4416 - acc: 0.9174 7776/10000 [======================>.......] - ETA: 0s - loss: 0.4423 - acc: 0.9172 8000/10000 [=======================>......] - ETA: 0s - loss: 0.4441 - acc: 0.9162 8224/10000 [=======================>......] - ETA: 0s - loss: 0.4437 - acc: 0.9160 8448/10000 [========================>.....] - ETA: 0s - loss: 0.4452 - acc: 0.9154 8672/10000 [=========================>....] - ETA: 0s - loss: 0.4488 - acc: 0.9143 8896/10000 [=========================>....] - ETA: 0s - loss: 0.4467 - acc: 0.9148 9120/10000 [==========================>...] - ETA: 0s - loss: 0.4475 - acc: 0.9148 9344/10000 [===========================>..] - ETA: 0s - loss: 0.4482 - acc: 0.9152 9568/10000 [===========================>..] - ETA: 0s - loss: 0.4477 - acc: 0.9149 9792/10000 [============================>.] - ETA: 0s - loss: 0.4452 - acc: 0.915610000/10000 [==============================] - 3s 264us/sample - loss: 0.4442 - acc: 0.9159
Test loss: 0.4442362295150757
Test accuracy: 0.9159
conv2d (3, 3, 3, 16)
conv2d_1 (3, 3, 16, 16)
conv2d_2 (3, 3, 16, 16)
conv2d_3 (3, 3, 16, 16)
conv2d_4 (3, 3, 16, 16)
conv2d_5 (3, 3, 16, 16)
conv2d_6 (3, 3, 16, 16)
conv2d_7 (3, 3, 16, 32)
conv2d_8 (3, 3, 32, 32)
conv2d_9 (1, 1, 16, 32)
conv2d_10 (3, 3, 32, 32)
conv2d_11 (3, 3, 32, 32)
conv2d_12 (3, 3, 32, 32)
conv2d_13 (3, 3, 32, 32)
conv2d_14 (3, 3, 32, 64)
conv2d_15 (3, 3, 64, 64)
conv2d_16 (1, 1, 32, 64)
conv2d_17 (3, 3, 64, 64)
conv2d_18 (3, 3, 64, 64)
conv2d_19 (3, 3, 64, 64)
conv2d_20 (3, 3, 64, 64)
error2 imsave() missing 1 required positional argument: 'arr'
error 2


PS:

Read file <149987.err> for stderr output of this job.

